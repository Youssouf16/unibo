{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading files and preparing the tweets (pre-processing is applied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rober\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rober\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\rober\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%run scripts/pre_processing.py\n",
    "df = pd.read_csv('data/tweet_df_class.csv', index_col='Datetime').drop('Unnamed: 0', axis=1)\n",
    "tweets_text = df.original_text\n",
    "prep_tweets = tweets_text.apply(pre_processing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating two datasets: one of \"malaria tweets\" and one of \"not malaria tweets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preproc'] = prep_tweets\n",
    "nodup_df = df.drop_duplicates(subset='original_text')\n",
    "tmp_tweets = nodup_df[['preproc', 'class']].reset_index(drop=True)\n",
    "cases_tweets = tmp_tweets[tmp_tweets['class']==1]\n",
    "not_cases_tweets = tmp_tweets[tmp_tweets['class']==0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a series with the most frequently used tokens in \"malaria tweets\" and their frequencies\n",
    "(bag of words is used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, bow_matrix = bag_of_words(cases_tweets.preproc)\n",
    "bow_df = pd.DataFrame(bow_matrix, columns = vocab)\n",
    "common_words = bow_df.sum(axis=0).sort_values()[-286:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the dataframe with oversampled tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the labeled common words dataframe\n",
    "words_df = pd.read_csv('data/common_words.csv')\n",
    "\n",
    "#creating a dictionary with key = part of sentence and value = dataframe of tokens related to that part of sentence\n",
    "#a is adjective, n is noun, v is verb and r is \"other\"\n",
    "words_split = {}\n",
    "for i, words in words_df.groupby('part'):\n",
    "    words.freq = words.freq/sum(words.freq) #you may change words.freq with the frequency of only test data tokens\n",
    "    words_split.update({str(i) : words.drop('part', axis=1)})\n",
    "\n",
    "#absolute frequencies of each category\n",
    "ss = words_df.groupby('part').sum()\n",
    "\n",
    "#creating a series of uncommon words (index) and their relative frequencies \n",
    "noise_words = bow_df.sum(axis=0).sort_values()[:-286] #here too you can use noises from only test data tokens\n",
    "noise_words /= sum(noise_words)\n",
    "\n",
    "#generating fake tweets from the oversampling function\n",
    "\n",
    "def fake_tweetter(n):\n",
    "\n",
    "    fake_tweets = pd.DataFrame(np.array([oversampler(words_split, noise_words) for i in range(n)]).T,\n",
    "            columns = ['preproc'])\n",
    "    fake_tweets.preproc = fake_tweets.preproc.str.split()\n",
    "    fake_tweets.insert(1, 'class', 1)\n",
    "\n",
    "    return fake_tweets\n",
    "\n",
    "#tweets with oversampling\n",
    "z = 1000\n",
    "oversampled_tweets = pd.concat([tmp_tweets, fake_tweetter(z)]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics function\n",
    "\n",
    "def metrics(test,pred):\n",
    "    \n",
    "    precision = precision_score(test[:], pred)\n",
    "    recall = recall_score(test[:], pred)\n",
    "    accuracy = accuracy_score(test[:], pred)\n",
    "    F1= ((2*recall*precision)/(recall+precision))\n",
    "    return precision, recall, accuracy, F1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slitting variables with target and dependent variables\n",
    "X = oversampled_tweets['preproc']\n",
    "y = oversampled_tweets['class']\n",
    "\n",
    "#bag of words\n",
    "vocab, X = bag_of_words(X)\n",
    "\n",
    "# Splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[:-z], y[:-z], test_size=0.3)\n",
    "\n",
    "#oversampling\n",
    "fakes = np.c_[X[-z:], y[-z:]]\n",
    "i_fakes = np.random.choice(range(z), len(y_train[y_train==0])-len(y_train[y_train==1]), replace=False)\n",
    "r_fakes = fakes[i_fakes]\n",
    "X_train = np.r_[X_train, r_fakes[:, :-1]]\n",
    "y_train = np.r_[y_train, r_fakes[:, -1]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.75 \n",
      "Precision: 0.80\n",
      "Recall: 0.82\n",
      "F1: 0.77\n"
     ]
    }
   ],
   "source": [
    "# Creating the Naive Bayes classifier\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Fit the classifier to the data\n",
    "nb.fit(X_train, y_train)\n",
    "nb_pred = nb.predict(X_test)\n",
    "NB_score = metrics(y_test,nb_pred)\n",
    "\n",
    "# score of the NB test\n",
    "print(\"\"\"Accuracy : {:.2f} \n",
    "Precision: {:.2f}\n",
    "Recall: {:.2f}\n",
    "F1: {:.2f}\"\"\".format(*NB_score))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.66 \n",
      "Precision: 0.88\n",
      "Recall: 0.78\n",
      "F1: 0.75\n"
     ]
    }
   ],
   "source": [
    "# training random forest classifier \n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "rf_score  = metrics(y_test,rf_pred)\n",
    "\n",
    "# score of the rf test\n",
    "print(\"\"\"Accuracy : {:.2f} \n",
    "Precision: {:.2f}\n",
    "Recall: {:.2f}\n",
    "F1: {:.2f}\"\"\".format(*rf_score))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.71 \n",
      "Precision: 0.84\n",
      "Recall: 0.81\n",
      "F1: 0.77\n"
     ]
    }
   ],
   "source": [
    "# training svm classifier SVM\n",
    "svm = svm.SVC(kernel='linear')\n",
    "svm.fit(X_train, y_train)\n",
    "svm_pred = svm.predict(X_test)\n",
    "svm_score  = metrics(y_test,svm_pred)\n",
    "\n",
    "# score of the svm test\n",
    "print(\"\"\"Accuracy : {:.2f} \n",
    "Precision: {:.2f}\n",
    "Recall: {:.2f}\n",
    "F1: {:.2f}\"\"\".format(*svm_score))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGB classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.68 \n",
      "Precision: 0.81\n",
      "Recall: 0.78\n",
      "F1: 0.74\n"
     ]
    }
   ],
   "source": [
    "# convert format to DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Definig parameters for XGBoost\n",
    "params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"eval_metric\": \"error\",\n",
    "    \"eta\": 0.1,\n",
    "    \"max_depth\": 10,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"seed\": 123\n",
    "}\n",
    "\n",
    "# training XGBoost model\n",
    "xgb = xgb.train(params, dtrain, num_boost_round=100)\n",
    "\n",
    "# predicting on testset\n",
    "xgb_pred1 = xgb.predict(dtest)\n",
    "xgb_pred = [round(value) for value in xgb_pred1]\n",
    "\n",
    "xgb_score = metrics(y_test,xgb_pred)\n",
    "print(\"\"\"Accuracy : {:.2f} \n",
    "Precision: {:.2f}\n",
    "Recall: {:.2f}\n",
    "F1: {:.2f}\"\"\".format(*xgb_score))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to do k-folds crossvalidation with oversampling and four metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_score(model, X, y, k):\n",
    "\n",
    "    #create k-folds (test)\n",
    "    folds = []\n",
    "    real = np.c_[X[:-z], y[:-z]]\n",
    "    for kf in range(k):\n",
    "        i_real = np.random.choice(range(real.shape[0]),int(X[:-z].shape[0]/k),replace=False)\n",
    "        r_real = real[i_real]\n",
    "        folds.append(r_real)\n",
    "        \n",
    "        #removing selected tweets\n",
    "        real = np.delete(real, i_real, axis=0)\n",
    "\n",
    "    #running models\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    F1s = []\n",
    "    for i in range(k):\n",
    "        test = folds[i]\n",
    "        train = np.vstack([folds[j] for j in range(k) if j!=i])\n",
    "        \n",
    "        #oversampling\n",
    "        fakes = np.c_[X[-z:], y[-z:]]\n",
    "        i_fakes = np.random.choice(range(z), len(train[:, -1][train[:, -1]==0])-len(train[:, -1][train[:, -1]==1]), replace=False)\n",
    "        r_fakes = fakes[i_fakes]\n",
    "        train = np.r_[train, r_fakes]\n",
    "        \n",
    "        #models\n",
    "        model.fit(train[:, :-1], train[:, -1])\n",
    "        pred = model.predict(test[:, :-1])\n",
    "\n",
    "        #metrics\n",
    "        precision = precision_score(test[:, -1], pred)\n",
    "        recall = recall_score(test[:, -1], pred)\n",
    "        accuracy = accuracy_score(test[:, -1], pred)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        accuracies.append(accuracy)\n",
    "        F1s.append((2*recall*precision)/(recall+precision))\n",
    "\n",
    "\n",
    "    return np.array(accuracies), np.array(precisions), np.array(recalls), np.array(F1s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.80 \n",
      "Precision: 0.68\n",
      "Recall: 0.82\n",
      "F1: 0.74\n"
     ]
    }
   ],
   "source": [
    "# training naive bayes classifier\n",
    "scores = cross_score(nb, X, y, 8)\n",
    "scores_means_nb = np.array([score.mean() for score in scores])\n",
    "\n",
    "print(\"\"\"Accuracy : {:.2f} \n",
    "Precision: {:.2f}\n",
    "Recall: {:.2f}\n",
    "F1: {:.2f}\"\"\".format(*scores_means_nb))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.76 \n",
      "Precision: 0.61\n",
      "Recall: 0.85\n",
      "F1: 0.71\n"
     ]
    }
   ],
   "source": [
    "# training random forest classifier \n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "scores = cross_score(rf, X, y, 8)\n",
    "scores_means_rf = np.array([score.mean() for score in scores])\n",
    "\n",
    "print(\"\"\"Accuracy : {:.2f} \n",
    "Precision: {:.2f}\n",
    "Recall: {:.2f}\n",
    "F1: {:.2f}\"\"\".format(*scores_means_rf))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM classifier with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.75 \n",
      "Precision: 0.61\n",
      "Recall: 0.81\n",
      "F1: 0.69\n"
     ]
    }
   ],
   "source": [
    "# training SVM with cv\n",
    "scores = cross_score(svm, X, y, 8)\n",
    "scores_means_svm = np.array([score.mean() for score in scores])\n",
    "\n",
    "print(\"\"\"Accuracy :  {:.2f} \n",
    "Precision: {:.2f}\n",
    "Recall: {:.2f}\n",
    "F1: {:.2f}\"\"\".format(*scores_means_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Accuracy  Precision  Recall   F1\n",
      "NB            0.75       0.80    0.82 0.77\n",
      "RF            0.66       0.88    0.78 0.75\n",
      "SVM           0.71       0.84    0.81 0.77\n",
      "XGB           0.68       0.81    0.78 0.74\n",
      "nbcross       0.80       0.68    0.82 0.74\n",
      "rfcross       0.76       0.61    0.85 0.71\n",
      "svmcross      0.75       0.61    0.81 0.69\n"
     ]
    }
   ],
   "source": [
    "#displaying all results\n",
    "classifiers_frame = pd.DataFrame([NB_score, rf_score, svm_score, xgb_score, scores_means_nb, scores_means_rf, scores_means_svm],\n",
    "                                index = ['NB', 'RF', 'SVM', 'XGB', 'nbcross', 'rfcross', 'svmcross'],\n",
    "                                columns = ['Accuracy', 'Precision', 'Recall', 'F1'])\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "print(classifiers_frame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "67783390838d2e02912aac04d405c75735cc05868bfeaabbe6ec2bbdb2e2542d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
