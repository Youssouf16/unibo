{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading files and preparing the tweets (pre-processing is applied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run scripts/pre_processing.py\n",
    "df = pd.read_csv('data/tweet_df_class.csv', index_col='Datetime').drop('Unnamed: 0', axis=1)\n",
    "tweets_text = df.original_text\n",
    "prep_tweets = tweets_text.apply(pre_processing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating two datasets: one of \"malaria tweets\" and one of \"not malaria tweets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preproc'] = prep_tweets\n",
    "nodup_df = df.drop_duplicates(subset='original_text')\n",
    "tmp_tweets = nodup_df[['preproc', 'class']].reset_index(drop=True)\n",
    "cases_tweets = tmp_tweets[tmp_tweets['class']==1]\n",
    "not_cases_tweets = tmp_tweets[tmp_tweets['class']==0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a series with the most frequently used tokens in \"malaria tweets\" and their frequencies\n",
    "(bag of words is used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, bow_matrix = bag_of_words(cases_tweets.preproc)\n",
    "bow_df = pd.DataFrame(bow_matrix, columns = vocab)\n",
    "common_words = bow_df.sum(axis=0).sort_values()[-286:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the labeled common words dataframe\n",
    "words_df = pd.read_csv('data/common_words.csv')\n",
    "\n",
    "#creating a dictionary with key = part of sentence and value = dataframe of tokens related to that part of sentence\n",
    "#a is adjective, n is noun, v is verb and r is \"other\"\n",
    "words_split = {}\n",
    "for i, words in words_df.groupby('part'):\n",
    "    words.freq = words.freq/sum(words.freq) #you may change words.freq with the frequency of only test data tokens\n",
    "    words_split.update({str(i) : words.drop('part', axis=1)})\n",
    "\n",
    "#absolute frequencies of each category\n",
    "ss = words_df.groupby('part').sum()\n",
    "\n",
    "#creating a series of uncommon words (index) and their relative frequencies \n",
    "noise_words = bow_df.sum(axis=0).sort_values()[:-286] #here too you can use noises from only test data tokens\n",
    "noise_words /= sum(noise_words)\n",
    "\n",
    "#generating fake tweets from the oversampling function\n",
    "fake_tweets = pd.DataFrame(np.array([oversampler(words_split, noise_words) for i in range(250)]).T, #change the 250 to choose how many\n",
    "            columns = ['preproc'])\n",
    "fake_tweets.preproc = fake_tweets.preproc.str.split()\n",
    "fake_tweets.insert(1, 'class', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is just to show how to concatenate everything \n",
    "training_tweets = pd.concat([tmp_tweets, fake_tweets]).reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "67783390838d2e02912aac04d405c75735cc05868bfeaabbe6ec2bbdb2e2542d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
