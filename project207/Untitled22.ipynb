{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5872376-e547-4403-b1a1-322b2e70d2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b750da63-3a7c-4bb2-a91b-92dbea4811ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "def remove_repeated_characters(tokens):\n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    match_substitution = r'\\1\\2\\3'\n",
    "    def replace(old_word):\n",
    "        if wordnet.synsets(old_word):\n",
    "            return old_word\n",
    "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "        return replace(new_word) if new_word != old_word else new_word\n",
    "            \n",
    "    correct_tokens = [replace(word) for word in tokens]\n",
    "    return correct_tokens\n",
    "\n",
    "\n",
    "def stopwords_removal(text):\n",
    "    wh_words = ['my', 'your', 'his', 'her', 'this', 'do', \"don't\"]\n",
    "    stop = set(stopwords.words('english'))\n",
    "    for word in wh_words:\n",
    "        stop.remove(word)\n",
    "    tweet = ' '.join([x for x in text.split() if x not in stop])\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def lemmatize(corpus):\n",
    "    lem = WordNetLemmatizer()\n",
    "    #corpus = [lem.lemmatize(x, pos = 'v') for x in corpus]\n",
    "    corpus = [lem.lemmatize(x, pos = 'r') for x in corpus]\n",
    "    corpus = [lem.lemmatize(x, pos = 'n') for x in corpus]\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4ac1080-a3fd-43c7-85e0-ef4b8cb83ec1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "def remove_repeated_characters(tokens):\n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    match_substitution = r'\\1\\2\\3'\n",
    "    def replace(old_word):\n",
    "        if wordnet.synsets(old_word):\n",
    "            return old_word\n",
    "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "        return replace(new_word) if new_word != old_word else new_word\n",
    "            \n",
    "    correct_tokens = [replace(word) for word in tokens]\n",
    "    return correct_tokens\n",
    "\n",
    "\n",
    "def stopwords_removal(text):\n",
    "    wh_words = ['my', 'your', 'his', 'her', 'this', 'do', \"don't\"]\n",
    "    stop = set(stopwords.words('english'))\n",
    "    for word in wh_words:\n",
    "        stop.remove(word)\n",
    "    tweet = ' '.join([x for x in text.split() if x not in stop])\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def lemmatize(corpus):\n",
    "    lem = WordNetLemmatizer()\n",
    "    #corpus = [lem.lemmatize(x, pos = 'v') for x in corpus]\n",
    "    corpus = [lem.lemmatize(x, pos = 'r') for x in corpus]\n",
    "    corpus = [lem.lemmatize(x, pos = 'n') for x in corpus]\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def stem(corpus, stem_type = None):\n",
    "    stemmer = SnowballStemmer(language='english')\n",
    "    corpus = [stemmer.stem(x) for x in corpus]\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def pre_processing(tweet):\n",
    "    \"\"\"\n",
    "    input: a string\n",
    "    output: tokenized and preprocessed text (a list)\n",
    "    \"\"\"\n",
    "\n",
    "    tweet = tweet.lower()\n",
    "\n",
    "    #changing urls with a unique word\n",
    "    p1 = re.compile(r'\\bhttps://t.co/\\w+')\n",
    "    p2 = re.compile(r'\\bhttps://^(t.co/\\w+)\\w')\n",
    "    tweet = re.sub(p1, 'tweeturl', tweet)\n",
    "    tweet = re.sub(p2, 'simpleurl', tweet)\n",
    "\n",
    "    #changing all spaces with a simple space and dots and commas\n",
    "    p3 = re.compile(r'\\s+')\n",
    "    p4 = re.compile(r\"[.,;'â€™-]\")\n",
    "    tweet = re.sub(p4, '', tweet)\n",
    "    tweet = re.sub(p3, ' ', tweet)\n",
    "\n",
    "\n",
    "    #removing stopwords\n",
    "    tweet = stopwords_removal(tweet)\n",
    "\n",
    "    #tokenize the tweet\n",
    "    tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    tweet = tokenizer.tokenize(tweet)\n",
    "    tweet = remove_repeated_characters(tweet)\n",
    "\n",
    "    #lemmatization and stemming\n",
    "    tweet = lemmatize(tweet)\n",
    "    tweet = stem(tweet)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "\n",
    "def bag_of_words(tweets):\n",
    "    \"\"\"\n",
    "    input:  a series of tokenized tweets (lists)\n",
    "    output: a vocabulary (list) and a bag of words (numpy array)\n",
    "    \"\"\"\n",
    "\n",
    "    set_of_words = set()\n",
    "    for tweet in tweets:\n",
    "        for word in tweet:\n",
    "            set_of_words.add(word)\n",
    "    vocab = list(set_of_words)\n",
    "\n",
    "    position = {}\n",
    "    for i, token in enumerate(vocab):\n",
    "        position[token] = i\n",
    "\n",
    "    bow_matrix = np.zeros((len(tweets), len(vocab)))\n",
    "\n",
    "    for i, preprocessed_sentence in enumerate(tweets):\n",
    "        for token in preprocessed_sentence:   \n",
    "            bow_matrix[i][position[token]] = bow_matrix[i][position[token]] + 1\n",
    "\n",
    "    return vocab, bow_matrix\n",
    "\n",
    "\n",
    "def oversampler(dict_of_words, noise):\n",
    "    \"\"\"\n",
    "    input: dict of words is a dictionary key: part of sentence - value: a dataframe of tokens and their relative frequency\n",
    "            noise is a series of relative frequencies, indexed by tokens\n",
    "    output: a random string\n",
    "    \"\"\"\n",
    "\n",
    "    adj = np.random.choice(dict_of_words['a'].tokens, 1, p=dict_of_words['a'].freq)\n",
    "    noun = np.random.choice(dict_of_words['n'].tokens, 3, p=dict_of_words['n'].freq)\n",
    "    verb = np.random.choice(dict_of_words['v'].tokens, 2, p=dict_of_words['v'].freq)\n",
    "    extra = np.random.choice(dict_of_words['r'].tokens, 3, p=dict_of_words['r'].freq)\n",
    "    noise = np.random.choice(noise.index, 3, p=noise.values)\n",
    "\n",
    "    \n",
    "    return ' '.join([noun[0],extra[0],verb[0],noun[1],extra[1],adj[0],noun[2],verb[1],extra[2]\\\n",
    "        ,noise[0],noise[1],noise[2]]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f94910c7-429d-4dc8-b460-db9f9428e079",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Documents/data/tweet_df_class.csv', index_col='Datetime').drop('Unnamed: 0', axis=1)\n",
    "tweets_text = df.original_text\n",
    "prep_tweets = tweets_text.apply(pre_processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76172563-82b1-4015-b2e0-585558dadce1",
   "metadata": {},
   "source": [
    "Creating two datasets: one of \"malaria tweets\" and one of \"not malaria tweets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cc72638-75bf-484b-8b54-3f9b18ad842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preproc'] = prep_tweets\n",
    "nodup_df = df.drop_duplicates(subset='original_text')\n",
    "tmp_tweets = nodup_df[['preproc', 'class']].reset_index(drop=True)\n",
    "cases_tweets = tmp_tweets[tmp_tweets['class']==1]\n",
    "not_cases_tweets = tmp_tweets[tmp_tweets['class']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06c1fe3-88df-4e27-bdba-8933aee0aca4",
   "metadata": {},
   "source": [
    "Creating a series with the most frequently used tokens in \"malaria tweets\" and their frequencies (bag of words is used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0896bb78-c52e-408b-9053-09fdec84825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, bow_matrix = bag_of_words(cases_tweets.preproc)\n",
    "bow_df = pd.DataFrame(bow_matrix, columns = vocab)\n",
    "common_words = bow_df.sum(axis=0).sort_values()[-286:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b129c2-44c6-419d-b9e3-83b55194e2b7",
   "metadata": {},
   "source": [
    "Creating the dataframe with oversampled tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8547ee4c-947a-4aa5-a7e1-f9b61d525e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the labeled common words dataframe\n",
    "words_df = pd.read_csv('Documents/data/common_words.csv')\n",
    "\n",
    "#creating a dictionary with key = part of sentence and value = dataframe of tokens related to that part of sentence\n",
    "#a is adjective, n is noun, v is verb and r is \"other\"\n",
    "words_split = {}\n",
    "for i, words in words_df.groupby('part'):\n",
    "    words.freq = words.freq/sum(words.freq) #you may change words.freq with the frequency of only test data tokens\n",
    "    words_split.update({str(i) : words.drop('part', axis=1)})\n",
    "\n",
    "#absolute frequencies of each category\n",
    "ss = words_df.groupby('part').sum()\n",
    "\n",
    "#creating a series of uncommon words (index) and their relative frequencies \n",
    "noise_words = bow_df.sum(axis=0).sort_values()[:-286] #here too you can use noises from only test data tokens\n",
    "noise_words /= sum(noise_words)\n",
    "\n",
    "#generating fake tweets from the oversampling function\n",
    "\n",
    "def fake_tweetter(n):\n",
    "\n",
    "    fake_tweets = pd.DataFrame(np.array([oversampler(words_split, noise_words) for i in range(n)]).T,\n",
    "            columns = ['preproc'])\n",
    "    fake_tweets.preproc = fake_tweets.preproc.str.split()\n",
    "    fake_tweets.insert(1, 'class', 1)\n",
    "\n",
    "    return fake_tweets\n",
    "\n",
    "#tweets with oversampling\n",
    "z = 1000\n",
    "oversampled_tweets = pd.concat([tmp_tweets, fake_tweetter(z)]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743ba3d4-c341-4990-9016-8daecaf09121",
   "metadata": {},
   "source": [
    "Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ec0528f-edfd-4cb3-aa78-d441976202d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "\n",
    "# slitting variables with target and dependent variables\n",
    "X = oversampled_tweets['preproc']\n",
    "y = oversampled_tweets['class']\n",
    "\n",
    "#bag of words\n",
    "vocab, X = bag_of_words(X)\n",
    "\n",
    "# Splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[:-z], y[:-z], test_size=0.3)\n",
    "\n",
    "#oversampling\n",
    "fakes = np.c_[X[-z:], y[-z:]]\n",
    "i_fakes = np.random.choice(range(z), len(y_train[y_train==0])-len(y_train[y_train==1]), replace=False)\n",
    "r_fakes = fakes[i_fakes]\n",
    "X_train = np.r_[X_train, r_fakes[:, :-1]]\n",
    "y_train = np.r_[y_train, r_fakes[:, -1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bdb7bc-ffd9-47de-b632-499813d3d9de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
